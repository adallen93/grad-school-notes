---
title: "B705 - Week 3 Lecture Notes"
date: "January 23, 2024"
author: "Austin Allen"
format:
  html:
    self-contained: true
    page-layout: full
    title-block-banner: true
    toc: true
    toc-depth: 3
    toc-location: body
    number-sections: false
    html-math-method: katex
    code-fold: true
    code-summary: "Show the code"
    code-overflow: wrap
    code-copy: hover
    code-tools:
        source: false
        toggle: true
        caption: See code
execute: 
  warning: false
    
---



::: {.panel-tabset}

### Tuesday


#### Questions from Last Time

* Why is the normality assumption not required to estimate the parameters in a linear model? I thought the error terms were assumed to be distributed normally with $\epsilon \sim N(0, \sigma^2)$
    - We don't need to assume normality to estimate parameters. We only need normality when we need to test linear associations

#### Lecture Notes


**Multiple Linear Regression**

* Recap
    - $Y_i - \bar{Y}$ can be broken up into two pieces:
        - $\hat{Y_i} - \bar{Y}$ is the regression piece
        - $Y_i - \hat{Y_i}$ is the error term
    - $\underbrace{\sum_{i = 1}^{n} (Y_i - \bar{Y})^2}_\text{SST} = \underbrace{\sum_{i = 1}^n (\hat{Y_i} - \bar{Y})^2}_\text{SSR} + \underbrace{\sum_{i = 1}^{n} (Y_i - \hat{Y_i})^2}_\text{SSE}$
    - An ANOVA table is always additive. The total row for degrees of freedom and summed squares is equal to the sum of the other rows for degrees and freedom and SS, respectively. Here's an example. Take the model: $Y_i = \beta_0 + \beta_1 X_{1_i} + \beta_2 X_{3_i} + \beta_3 X_{4_i} + \epsilon_i$ with a sample size of 100. We might have the following table:

|    |df |SS| MS |
|----|---|---|----|
|Model|3 |280|280/3|
|Error|96|220|220/96|
|Total|99|500|500/1|


* Interpretation of multiple regression:
  - The model: $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p + \epsilon$

\begin{align*}
$\hat{Y} &= X \hat{\beta}\\
&= X(X^TX)^-1X^TY\\
\hat{Y} &= HY
\end{align*}




:::
