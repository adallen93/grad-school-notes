---
title: "B705 - Week 3 Lecture Notes"
date: "January 23, 2024"
author: "Austin Allen"
format:
  html:
    self-contained: true
    page-layout: full
    title-block-banner: true
    toc: true
    toc-depth: 3
    toc-location: body
    number-sections: false
    html-math-method: katex
    code-fold: true
    code-summary: "Show the code"
    code-overflow: wrap
    code-copy: hover
    code-tools:
        source: false
        toggle: true
        caption: See code
execute: 
  warning: false
    
---



::: {.panel-tabset}

### Tuesday


#### Questions from Last Time

* Why is the normality assumption not required to estimate the parameters in a linear model? I thought the error terms were assumed to be distributed normally with $\epsilon \sim N(0, \sigma^2)$
    - We don't need to assume normality to estimate parameters. We only need normality when we need to test linear associations

#### Lecture Notes


**Multiple Linear Regression**

* Recap
    - $Y_i - \bar{Y}$ can be broken up into two pieces:
        - $\hat{Y_i} - \bar{Y}$ is the regression piece
        - $Y_i - \hat{Y_i}$ is the error term
    - $\underbrace{\sum_{i = 1}^{n} (Y_i - \bar{Y})^2}_\text{SST} = \underbrace{\sum_{i = 1}^n (\hat{Y_i} - \bar{Y})^2}_\text{SSR} + \underbrace{\sum_{i = 1}^{n} (Y_i - \hat{Y_i})^2}_\text{SSE}$
    - An ANOVA table is always additive. The total row for degrees of freedom and summed squares is equal to the sum of the other rows for degrees and freedom and SS, respectively. Here's an example. Take the model: $Y_i = \beta_0 + \beta_1 X_{1_i} + \beta_2 X_{3_i} + \beta_3 X_{4_i} + \epsilon_i$ with a sample size of 100. We might have the following table:

|    |df |SS| MS |
|----|---|---|----|
|Model|3 |280|280/3|
|Error|96|220|220/96|
|Total|99|500|500/1|


* Interpretation of multiple regression:
  - The model: $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p + \epsilon$

\begin{align*}
$\hat{Y} &= X \hat{\beta}\\
&= X(X^TX)^-1X^TY\\
\hat{Y} &= HY
\end{align*}

* Assumptions
  - Linearity: $E(\epsilon_i) = 0$, which implies that $E(Y_i) = \beta_0 + \beta_1 X_{1_i} + ... + \beta_p X_{p_i}$
  - Homoscedasticity: $Var(\epsilon_i) = \sigma^2_\epsilon$, which implies that $Var(Y_i|X_{1_i}, ... , X_{p_i}) = \sigma^2_\epsilon$
  - Normality: $\epsilon \sim N(0, \sigma^2)$, which implies that $Y_i | X_{1_i},...,X_{p_i} \sim N(\beta_0 + \beta_1 X_{1_i} + ... + \beta_p X_{p_i}, \sigma^2_\epsilon)$
  - Independence: $\epsilon_i \epsilon_j$ are independent for $i \ne j$. This means that all $Y$ values are independent of each other
* Whenever you run an analysis 
* Test statistic for global hypothesis: $F = \frac{\text{SSR}/p}{\text{SSE}/(n-p-1)} \sim F(p, n-p-1)$



### Thursday


#### Lecture Notes

* If you have an ANOVA table, the bottom row of the MS column is not that useful. It is essential $S^2Y$
* $F = \frac{MSR}{MSE} \sim F(\text{Model DF}, \text{Error DF})$
* Matrix form: $\underbrace{Y}_{nx1} = \underbrace{X}_{\text{n x [p + 1]}} \overbrace{\beta}_{\text{n x [p + 1]}} + \underbrace{\epsilon}_{\text{n x 1}}$
  - p is the number of predictors in the model. It is a n x [p + 1] matrix because there is an extra column for $\beta_0$
  - $X$ is the called the design matrix. It's first column is all 1's that correspond to the intercept. The rest of the columns correspond to $\beta_1,...,\beta_p$
* In the output for a lm summary in R, the residual standard error is $SY.X$. I think I get this. It's the same thing as $\hat{\sigma_\epsilon}$. The residual standard error is the estimate for the standard deviation for $Y_i$ given the model, which is the same as the estimate for the standard deviation of $\epsilon_i$
* Again the F-statistic is testing the global hypothesis, which is that $\beta_1 = ... = \beta_p = 0$


:::
