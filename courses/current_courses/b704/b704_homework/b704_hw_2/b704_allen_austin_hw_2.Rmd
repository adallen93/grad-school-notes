---
title: "BIOSTAT 704 - Homework 2"
author: "Austin Allen"
date: "February 6, 2024"
output:
    pdf_document:
        latex_engine: xelatex
---

```{r include = FALSE}
library(tidyverse)
library(knitr)
library(pander)
opts_chunk$set(warning = FALSE, message = FALSE, fig.align = 'center', fig.width = 4, fig.height = 3)

```

## Problem 1



Let $Y_1,...,Y_n$ be a random sample where $Y_i \sim \text{GAM}(\theta, \kappa)$.

* $Y_i$ represents the amount of rainfall accumulated in a reservoir over a year
* $\mu = E(YPi) = 132$
* $\sigma^2 = Var(Y_i) = 26.4$
   $\implies \sigma = 5.14$
* $Y_n = 1/n \sum_{i = 1}^{n} X_i$
  * Note: I'm not sure if this was to be $Y_n = 1/n \sum_{i = 1}^{n} Y_i$

**Part (a):** Give an approximation of $P(\bar{Y} > 200)$ for $n = 500$



> To calculate this probability, we're going to rely on the central limit theorem. We will let $Z = \frac{\bar{Y}_n - \mu}{\sigma/\sqrt{n}} =  \frac{\bar{Y}_n - 132}{5.14/\sqrt{500}}$.

\begin{align*}
P(\bar{Y}_n \ge 200) &= P(\frac{\bar{Y}_n - 132}{0.23} \ge \frac{200 - 132}{0.23})\\
&= 1 - P(\frac{\bar{Y}_n - 132}{0.23} < \frac{200 - 132}{0.23})\\
&= 1 - P(Z < 295.9)\\
&= 0
\end{align*}

**Part (b):** What is the true distribution of $\bar{Y}_n$?

> I decided to use the MGF method to deterimine this distribution:

\begin{align*}
M_{Y_i}(t) &= E(exp(Y_it))\\
&= (\frac{1}{1 - \theta t})^{\kappa}\\
M_{\bar{Y}_n}(t) &= E(exp(\bar{Y}_n t))\\
&= E(exp((1/n)\sum Y_i t))\\
&= E(exp((t/n)\sum Y_i))\\
&= E(\prod exp((Y_i t)/n))\\
&= \prod E(exp((Y_i t)/n)))\\
&= \prod M_{Y_i}(t/n)\\
&= (\frac{1}{1 - \theta t/n})^{n\kappa}
\end{align*}

> You can see that $\bar{Y}_n$ follows a Gamma distribution with parameters $GAM(\theta/n, n\kappa)$.

**Part (c):** Does $\bar{Y}_n$ converge in probability? If so, what is the limit?


> Let's start with the definition of *stochastic convergence*. That is, $Y_N$ is said to converge in probability (stochastically) to a constant c if and only if for every $\epsilon > 0$, $\lim_{n \to \infty} P[|Y_n- c| < \epsilon] = 1$ (see chapter 7 notes, page 17).
>
> We can also remember the Chebychev inequality to help us determine stochastic convergence. Remember that probabilities are bounded by 0 and 1, which when combined with the Chebychev inequality, gives rise to the following:

<!-- \begin{align*} -->
<!-- &0 \le P[|\bar{Y}_n - E(\bar{Y}_n) \ge \epsilon] \le \frac{Var(\bar{Y}_n)}{\epsilon^2}\\ -->
<!-- &\implies 0 \le P[|\bar{Y}_n - \mu| \ge \epsilon] \le \frac{\sqrt{26.4/n}}{\epsilon^2}\\ -->
<!-- &\implies 0 \le \lim_{n \to \infty} P[|\bar{Y}_n - \mu| \ge \epsilon]  \le \lim_{n \to \infty} \frac{\sqrt{26.4/n}}{\epsilon^2}\\ -->
<!-- &\implies 0 \le P[|\bar{Y}_n - \mu| \ge \epsilon] \le 0\\ -->
<!-- &\implies 1 - 0 \ge 1 - P[|\bar{Y}_n - \mu| \ge \epsilon] \ge 1 - 0\\ -->
<!-- &\implies 1 \ge P[|\bar{Y}_n - \mu| < \epsilon] \ge 1 -->
<!-- \end{align*} -->

<!-- > We have proved by the sqeeze theorem that the limit as $n \rightarrow \infty$ of this probability is 1. Thus, by the definition of stochastic convergence, $\bar{Y}_n$ converges stochastically to $\mu$ which is 132. -->

<!-- **Part (d):** Calculate the true probability $P(|\bar{Y}_n - \mu| \le 0.4)$ for $n = 20$. Explain what $|bar{Y}_n - \mu| \le 0.4$ means in terms of the amount of rainfall, and interpret the above result. -->

<!-- > I'm going to put this in a form that will be easy to calculate with a software tool like `R`: -->

<!-- \begin{align*} -->
<!-- P(|\bar{Y}_n - \mu| \le 0.4) &= P(\bar{Y}_n  - \mu \le 0.4) - P(\bar{Y}_n - \mu \le -0.4)\\ -->
<!-- &= P(\bar{Y}_n  \le \mu + 0.4) - P(\bar{Y}_n \le \mu  -0.4)\\ -->
<!-- &= P(\bar{Y}_n  \le 132.4) - P(\bar{Y}_n \le 131.6)\\ -->
<!-- \end{align*} -->

<!-- > This is a nice form because the `pgamma()` function in `R` calculates the CDF at a given point $q$, $F_X(q)$. Let's solve for $\kappa$ and $theta$ to determine what parameters to pass into the function: -->



<!-- \begin{align*} -->
<!-- &E(\bar{Y}_N) = (\theta/n)(n \kappa)\\ -->
<!-- &\implies 132 = (\theta/20)(20\kappa)\\ -->
<!-- &\implies \theta = 132/\kappa\\ -->
<!-- &Var(\bar{Y}_n) = (\theta/n)^2(n\kappa)\\ -->
<!-- &\implies \frac{26.4}{20} = \frac{\kappa \theta^2}{20}\\ -->
<!-- &\implies \frac{26.4}{20} = \frac{\kappa (132/\kappa)^2}{20}\\ -->
<!-- &\implies 26.4 = \kappa (132/\kappa)^2\\ -->
<!-- &\implies 26.4\kappa =  132^2\\ -->
<!-- &\implies \kappa =  132^2/26.4\\ -->
<!-- &\implies \kappa =  660\\ -->
<!-- & \theta = 132/\kappa\\ -->
<!-- &\implies \theta = 132/660 -->
<!-- \end{align*} -->

<!-- > Thus, we have determined that $\kappa = 660$ and $\theta = 132/660$, which implies that the parameters for the function need to be `scale = 1/100, shape = 13200`. Let's compute this in `R`: -->


<!-- ```{r} -->
<!-- # Compute probability -->
<!-- prob <- pgamma(q = 132.4, scale = 1/100, shape = 13200) - pgamma(q = 131.6, scale = 1/100, shape = 13200) -->
<!-- # Print rounded probability -->
<!-- print(round(prob, 3)) -->

<!-- ``` -->

<!-- > The answer is 0.272. To understand what this probability means, let's dive into the expression $|bar{Y}_n - \mu| \le 0.4$.  -->
<!-- >  -->
<!-- > We know the $Y_i$ is the amount of rainfall accumulated over the span of a year in a reservoir. Thus, $\mu$ is the mean of accumulated rainfall in the reservoir over all years, and $\bar{Y}_n$ is the average rainfall of a random sample of years. Thus, the absolute value of the difference between the average of a random sample of years of rainfall and the mean rainfall is how far we're likely to see the sample mean to be from the population mean. Stated another way, if we take a random sample of 20 years and measure the accumulated rainfall, we would expect the average of the sample to be close to the true mean of 132 inches of accumulated rainfall in the reservoir. In fact, the probability that the sample mean is closer to the true mean than 0.4 inches is 0.272. -->

<!-- **Part (e):** Find $P(|\bar{Y}_n - \mu| \le 0.4)$ when $n = 25,50,100,200$. What patter do you see emerge fo this probability as $n$ gets larger?  -->

<!-- > I'm going to do this in `R` to create a nice visual of how the probability changes as the sample size increase.  -->

<!-- ```{r} -->
<!-- # Vectorize probabilities -->
<!-- n <- c(25, 50, 100, 200) -->
<!-- theta <- (1/5)/n -->
<!-- kappa <- 660*n -->
<!-- probs <- pgamma(q = 132.4, scale = theta, shape = kappa) - pgamma(q = 131.6, scale = theta, shape = kappa) -->
<!-- # Create dataframe with probabilities for values of n -->
<!-- df <- data.frame(n = n, y = probs) -->
<!-- # Create plot to display pattern -->
<!-- ggplot(df) + -->
<!--   geom_point(aes(x = n, y = y), size = 2, col = "skyblue") + -->
<!--   labs(x = "Sample Size (n)", y = "P(|Yn - E(Y)| < 0.4)", title = "How does probability change as sample size increases?") + -->
<!--   theme_bw() + -->
<!--   theme(text = element_text(family = "serif"), -->
<!--         plot.title = element_text(hjust = .5, face = "bold"), -->
<!--         panel.grid = element_blank()) -->
<!-- ``` -->

<!-- > This plot aligns with our intuition. We know that the variance of the sampling distribution of $\bar{Y}$ will decrease as $n$ gets larger. Thus, the probability that our sample mean will be close to $\mu$ increases as the sample size increases. -->

<!-- **Part (f):** Now suppose $\sigma^2 = 79.3$. Recalculate the probabilities in part (c). Do the results change drastically? How do you explain such a trend?  -->

<!-- > **Note:** I'm assuming that this is asking to compare with the probabilities calculated in part (e), not part (c) as part (c) concerns convergence in probability. The variance is dependent on sample size in that problem, so the results will be the same as $n \rightarrow \infty$.  -->

<!-- > Let's quickly recalculate our original parameters for $Y_i$ based on the updated variance.  -->

<!-- \begin{align*} -->
<!-- &E(\bar{Y}_N) = (\theta/n)(n \kappa)\\ -->
<!-- &\implies \theta = 132/\kappa\\ -->
<!-- &Var(\bar{Y}_n) = (\theta/n)^2(n\kappa)\\ -->
<!-- &\implies \frac{79.3}{n} = \frac{\kappa \theta^2}{n}\\ -->
<!-- &\implies \frac{79.3}{n} = \frac{\kappa (132/\kappa)^2}{n}\\ -->
<!-- &\implies 79.3 = \kappa (132/\kappa)^2\\ -->
<!-- &\implies 79.3 \kappa =  132^2\\ -->
<!-- &\implies \kappa =  132^2/79.3\\ -->
<!-- &\implies \kappa =  219.7\\ -->
<!-- & \theta = 132/\kappa\\ -->
<!-- &\implies \theta = 0.6\\ -->
<!-- \end{align*} -->

<!-- > Now, let's combine this result with the code in the previous example to see how the trend changes.  -->

<!-- ```{r} -->
<!-- # Vectorize probabilities -->
<!-- n <- c(25, 50, 100, 200) -->
<!-- theta <- (0.6)/n -->
<!-- kappa <- 219.7226*n -->
<!-- probs <- pgamma(q = 132.4, scale = theta, shape = kappa) - pgamma(q = 131.6, scale = theta, shape = kappa) -->
<!-- # Create dataframe with probabilities for values of n -->
<!-- df <- data.frame(n = n, y = probs) -->
<!-- # Create plot to display pattern -->
<!-- ggplot(df) + -->
<!--   geom_point(aes(x = n, y = y), size = 2, col = "skyblue") + -->
<!--   labs(x = "Sample Size (n)", y = "P(|Yn - E(Y)| < 0.4)", title = "Did the trend change when variance increased?") + -->
<!--   theme_bw() + -->
<!--   theme(text = element_text(family = "serif"), -->
<!--         plot.title = element_text(hjust = .5, face = "bold"), -->
<!--         panel.grid = element_blank()) -->
<!-- ``` -->

<!-- > You can see that the probabilities were nearly cut in half. Even with a sample size of 200, the probability of $\bar{Y}_n$ being within 0.4 inches of $\mu$ is less than 0.5! This result actually does align with our intuition. Because we're dealing with a higher variance in this problem, we would expect there to be more variability in the sampling distribution as well, leading to values farther from $\mu$.  -->

<!-- **Part (g):** Compared to $P(|\bar{Y}_n - \mu| \le 0.4)$, will the probability $P(|\bar{Y}_n - \mu| \le \epsilon)$ increase or decrease for $\epsilon < 0.2, n = 50, \sigma^2 = 26.4$. Justify your answer.  -->

<!-- > Here's what I think this problem is saying I believe it is asking us to determine either by logic or proof whether having a smaller value for $\epsilon$ increases or decreases the probability $P(|\bar{Y}_n - \mu| \le \epsilon)$.  -->
<!-- >  -->
<!-- > Because we are given some information from the problem statement, I'm going to calculate the two probabilities for $n = 50$ and $\sigma^2 = 26.4$.  -->

<!-- ```{r} -->
<!-- # Calculate probability for epsilon = 0.4 -->
<!-- prob_0.4 <- pgamma(q = 132.4, scale = ((1/5)/50), shape = (660*50)) - pgamma(q = 131.6, scale = 1/100, shape = 13200) -->
<!-- # Calculate probability for epsilon = 0.2 -->
<!-- prob_0.2 <- pgamma(q = 132.2, scale = ((1/5)/50), shape = (660*50)) - pgamma(q = 131.8, scale = 1/100, shape = 13200) -->
<!-- # print probabilities -->
<!-- print(paste("The probability for epsilon = 0.4 is", round(prob_0.4, 3), "and the probability for epsilon = 0.2 is", round(prob_0.2, 3))) -->

<!-- ``` -->

<!-- > Not surprisingly, the probability decreases when we lower the value of $\epsilon$, because we're narrowing the range, meaning that less sample means will be within the range, holding $\sigma^2$ and $n$ constant.  -->

## Problem 2

Suppose that $X_n$ follows a binomial distribution with parameters $(n, \pi)$ where $\pi \in (0,1)$. Define $Y_n = \frac{1}{n}X_n$ for each $n$.

In each case below, clearly indicate which theorems you have used to establish the result.

**Hint:** First, write **Y_n** as a sample mean of a random sample that you will specify.

> We can specify $Y_n$ as a sample mean by considering that $X_n = \sum_{i = 1}^{n} W_i, W_i \sim Binom(1, \pi)$. This will help us in the following parts.


**Part (a):** Find the asymptotic normal distribution of $Y_n$.

> By the central limit theorem and definition 7.5.1, if it can be shown that $\frac{Y_n - m}{c/\sqrt{n}} \sim N(0,1)$ as $n \rightarrow \infty$, then the asymptotic normal distribution of $Y_n$ is $Y_n \sim^d N(m, c^2/n)$.
>
> Because we're dealing with the sample mean of $n$ $i.i.d.$ random variables of $W_i$, where $W_i \sim Binom(1,\pi)$, we know that the asymptotic normal distribution of $Y_n$ is $N(\pi, \frac{\pi(1 - \pi)}{n})$.


**Part (b):** Find the asymptotic normal distribution of $\ln(Y_n)$.




## Problem 3


Let $W_i$ be the weight of the $i$th airline passenger's luggage. Assume that the weights are independent, each with pdf $f(w) = \theta B^{-\theta} w^{\theta-1}, 0 < w < B, \quad 0 \quad o.w.$

**Part (a):** For $n = 100, \theta = 3, B = 80$, approximate $P\left[\sum_{i = 1}^{100} W_i > 6025\right]$


> By the CLT, we know that $\sum W_i$ is approximately distributed normally with parameters N(n\mu, n \sigma^2). Using an online integral calculator, I calculated $E(W_i)$ to be 60 and $Var(W_i)$ to be 240. Thus, this sum is approximated as follows: $\sum_{i = 1}^{n}


**Part (b):** If W_{1:n} is the smallest value of n, 

## Problem 4

$E(X_{1:n}) = \int_{0}^{\infty} [1 - F_X(x)]^{n - 1}dF(x)$
